{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9dc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Scikit-learn & LightGBM Imports ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec1273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_metric(y_true, y_pred):\n",
    "    \"\"\"Custom SMAPE metric for LightGBM training.\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    denominator[denominator == 0] = 1e-6\n",
    "    smape_val = np.mean(numerator / denominator) * 100\n",
    "    # LightGBM requires (metric_name, value, is_higher_better)\n",
    "    return 'smape', smape_val, False\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates SMAPE for final evaluation.\"\"\"\n",
    "    return smape_metric(y_true, y_pred)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c75a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Text Feature Engineering Functions ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_text_features(df):\n",
    "    df['item_name'] = df['catalog_content'].str.extract(r\"Item Name: (.*?)\\n\").iloc[:, 0].fillna('')\n",
    "    df['description'] = df['catalog_content'].str.extract(r\"Product Description: (.*?)\\nValue:\").iloc[:, 0].fillna('')\n",
    "    bullet_points_list = df['catalog_content'].str.findall(r\"Bullet Point \\d+: (.*)\")\n",
    "    df['bullet_points'] = bullet_points_list.apply(lambda x: ' '.join(x))\n",
    "    df['quantity_value'] = df['catalog_content'].str.extract(r\"Value: ([\\d.]+)\").astype(float)\n",
    "    pack_size = df['catalog_content'].str.extract(r\"\\(Pack of (\\d+)\\)\").iloc[:, 0].astype(float)\n",
    "    df['normalized_quantity'] = df['quantity_value'].fillna(1) * pack_size.fillna(1)\n",
    "    df['brand'] = df['item_name'].str.split().str[0]\n",
    "    df['num_bullet_points'] = bullet_points_list.apply(len)\n",
    "    df['item_name_length'] = df['item_name'].str.len()\n",
    "    df['description_length'] = df['description'].str.len()\n",
    "    full_text = df['item_name'] + ' ' + df['bullet_points'] + ' ' + df['description']\n",
    "    df['processed_text'] = full_text.apply(clean_text)\n",
    "    keywords = ['premium', 'organic', 'gluten free', 'vegan', 'natural', 'non-gmo']\n",
    "    for keyword in keywords:\n",
    "        df[f'is_{keyword.replace(\" \", \"_\")}'] = df['processed_text'].str.contains(keyword).astype(int)\n",
    "    df.fillna({'normalized_quantity': 1, 'item_name_length': 0, 'description_length': 0}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0d3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PyTorch U-Net Encoder for Image Features ---\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True), nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv_block(x)\n",
    "\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.c1, self.p1 = ConvBlock(in_channels, 16), nn.MaxPool2d(2)\n",
    "        self.c2, self.p2 = ConvBlock(16, 32), nn.MaxPool2d(2)\n",
    "        self.c3, self.p3 = ConvBlock(32, 64, 0.2), nn.MaxPool2d(2)\n",
    "        self.c4, self.p4 = ConvBlock(64, 128, 0.2), nn.MaxPool2d(2)\n",
    "        self.c5 = ConvBlock(128, 256, 0.3)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.p1(self.c1(x)); x = self.p2(self.c2(x)); x = self.p3(self.c3(x))\n",
    "        x = self.p4(self.c4(x)); x = self.c5(x)\n",
    "        return torch.flatten(self.pool(x), 1)\n",
    "\n",
    "def load_and_preprocess_image(image_path, transform):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    except (FileNotFoundError, OSError):\n",
    "        return None\n",
    "\n",
    "def create_image_features(df, image_folder_path, encoder_model, device):\n",
    "    encoder_model.eval().to(device)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for sample_id in tqdm(df['sample_id'], desc=f\"Processing Images in {image_folder_path}\"):\n",
    "            image_path = os.path.join(image_folder_path, f\"{sample_id}.jpg\")\n",
    "            img_tensor = load_and_preprocess_image(image_path, transform)\n",
    "            if img_tensor is not None:\n",
    "                features = encoder_model(img_tensor.to(device))\n",
    "                all_features.append(features.cpu().numpy()[0])\n",
    "            else:\n",
    "                all_features.append(np.zeros(256))\n",
    "    return np.array(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423f5079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing text features...\n",
      "\n",
      "Processing image features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images in C:\\Amazon ML Challange\\train_images: 100%|██████████| 75000/75000 [1:28:13<00:00, 14.17it/s]\n",
      "Processing Images in C:\\Amazon ML Challange\\student_resource\\test_images: 100%|██████████| 75000/75000 [1:22:46<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining text and image features...\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE_DIR = r'C:\\Amazon ML Challange\\train_images'\n",
    "TEST_IMAGE_DIR = r'C:\\Amazon ML Challange\\student_resource\\test_images'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('C:\\\\Amazon ML Challange\\\\student_resource\\\\dataset\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Amazon ML Challange\\\\student_resource\\\\dataset\\\\test.csv')\n",
    "y_train = train_df['price']\n",
    "test_ids = test_df['sample_id']\n",
    "\n",
    "print(\"Processing text features...\")\n",
    "train_df_text = extract_text_features(train_df.copy())\n",
    "test_df_text = extract_text_features(test_df.copy())\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "train_text_vec = tfidf.fit_transform(train_df_text['processed_text'])\n",
    "test_text_vec = tfidf.transform(test_df_text['processed_text'])\n",
    "\n",
    "num_cols = [c for c in train_df_text.columns if train_df_text[c].dtype != 'object' and c not in ['sample_id', 'price', 'catalog_content']]\n",
    "train_text_features = hstack([train_text_vec, csr_matrix(train_df_text[num_cols].values)])\n",
    "test_text_features = hstack([test_text_vec, csr_matrix(test_df_text[num_cols].values)])\n",
    "\n",
    "print(\"\\nProcessing image features...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = UNetEncoder()\n",
    "train_image_features = create_image_features(train_df, TRAIN_IMAGE_DIR, encoder, device)\n",
    "test_image_features = create_image_features(test_df, TEST_IMAGE_DIR, encoder, device)\n",
    "\n",
    "print(\"\\nCombining text and image features...\")\n",
    "X_train_full = hstack([train_text_features, csr_matrix(train_image_features)]).tocsr()\n",
    "X_test = hstack([test_text_features, csr_matrix(test_image_features)]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e1e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data for validation...\n",
      "Training set shape: (60000, 5267), Validation set shape: (15000, 5267)\n",
      "\n",
      "Training LightGBM model with validation...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.614596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 718380\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 5266\n",
      "[LightGBM] [Info] Start training from score 14.090000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's l1: 15.4516\tvalid_0's smape: 67.1152\n",
      "[20]\tvalid_0's l1: 14.735\tvalid_0's smape: 64.2166\n",
      "[30]\tvalid_0's l1: 14.2696\tvalid_0's smape: 62.444\n",
      "[40]\tvalid_0's l1: 13.9534\tvalid_0's smape: 61.2182\n",
      "[50]\tvalid_0's l1: 13.6907\tvalid_0's smape: 60.1931\n",
      "[60]\tvalid_0's l1: 13.4843\tvalid_0's smape: 59.394\n",
      "[70]\tvalid_0's l1: 13.3359\tvalid_0's smape: 58.7905\n",
      "[80]\tvalid_0's l1: 13.2086\tvalid_0's smape: 58.261\n",
      "[90]\tvalid_0's l1: 13.1101\tvalid_0's smape: 57.8889\n",
      "[100]\tvalid_0's l1: 13.0325\tvalid_0's smape: 57.5667\n",
      "[110]\tvalid_0's l1: 12.9434\tvalid_0's smape: 57.2524\n",
      "[120]\tvalid_0's l1: 12.8862\tvalid_0's smape: 57.0495\n",
      "[130]\tvalid_0's l1: 12.825\tvalid_0's smape: 56.8396\n",
      "[140]\tvalid_0's l1: 12.7688\tvalid_0's smape: 56.6437\n",
      "[150]\tvalid_0's l1: 12.7209\tvalid_0's smape: 56.4779\n",
      "[160]\tvalid_0's l1: 12.6791\tvalid_0's smape: 56.3479\n",
      "[170]\tvalid_0's l1: 12.6445\tvalid_0's smape: 56.223\n",
      "[180]\tvalid_0's l1: 12.6092\tvalid_0's smape: 56.1055\n",
      "[190]\tvalid_0's l1: 12.5788\tvalid_0's smape: 55.9795\n",
      "[200]\tvalid_0's l1: 12.5516\tvalid_0's smape: 55.8712\n",
      "[210]\tvalid_0's l1: 12.5309\tvalid_0's smape: 55.7994\n",
      "[220]\tvalid_0's l1: 12.5146\tvalid_0's smape: 55.7215\n",
      "[230]\tvalid_0's l1: 12.4987\tvalid_0's smape: 55.6498\n",
      "[240]\tvalid_0's l1: 12.481\tvalid_0's smape: 55.5922\n",
      "[250]\tvalid_0's l1: 12.4636\tvalid_0's smape: 55.527\n",
      "[260]\tvalid_0's l1: 12.4452\tvalid_0's smape: 55.4767\n",
      "[270]\tvalid_0's l1: 12.4334\tvalid_0's smape: 55.4397\n",
      "[280]\tvalid_0's l1: 12.4219\tvalid_0's smape: 55.3894\n",
      "[290]\tvalid_0's l1: 12.4125\tvalid_0's smape: 55.3564\n",
      "[300]\tvalid_0's l1: 12.4048\tvalid_0's smape: 55.3216\n",
      "[310]\tvalid_0's l1: 12.3905\tvalid_0's smape: 55.2614\n",
      "[320]\tvalid_0's l1: 12.379\tvalid_0's smape: 55.2054\n",
      "[330]\tvalid_0's l1: 12.3718\tvalid_0's smape: 55.1751\n",
      "[340]\tvalid_0's l1: 12.3622\tvalid_0's smape: 55.1226\n",
      "[350]\tvalid_0's l1: 12.3556\tvalid_0's smape: 55.0941\n",
      "[360]\tvalid_0's l1: 12.3476\tvalid_0's smape: 55.0508\n",
      "[370]\tvalid_0's l1: 12.3422\tvalid_0's smape: 55.0233\n",
      "[380]\tvalid_0's l1: 12.3354\tvalid_0's smape: 54.995\n",
      "[390]\tvalid_0's l1: 12.331\tvalid_0's smape: 54.9695\n",
      "[400]\tvalid_0's l1: 12.3251\tvalid_0's smape: 54.9449\n",
      "[410]\tvalid_0's l1: 12.3172\tvalid_0's smape: 54.9258\n",
      "[420]\tvalid_0's l1: 12.3122\tvalid_0's smape: 54.8995\n",
      "[430]\tvalid_0's l1: 12.3055\tvalid_0's smape: 54.8724\n",
      "[440]\tvalid_0's l1: 12.3005\tvalid_0's smape: 54.8594\n",
      "[450]\tvalid_0's l1: 12.2891\tvalid_0's smape: 54.8179\n",
      "[460]\tvalid_0's l1: 12.2843\tvalid_0's smape: 54.8098\n",
      "[470]\tvalid_0's l1: 12.2795\tvalid_0's smape: 54.7842\n",
      "[480]\tvalid_0's l1: 12.2738\tvalid_0's smape: 54.775\n",
      "[490]\tvalid_0's l1: 12.2712\tvalid_0's smape: 54.7683\n",
      "[500]\tvalid_0's l1: 12.267\tvalid_0's smape: 54.7491\n",
      "[510]\tvalid_0's l1: 12.2607\tvalid_0's smape: 54.7065\n",
      "[520]\tvalid_0's l1: 12.2561\tvalid_0's smape: 54.6863\n",
      "[530]\tvalid_0's l1: 12.2495\tvalid_0's smape: 54.66\n",
      "[540]\tvalid_0's l1: 12.2463\tvalid_0's smape: 54.6572\n",
      "[550]\tvalid_0's l1: 12.2388\tvalid_0's smape: 54.6276\n",
      "[560]\tvalid_0's l1: 12.2333\tvalid_0's smape: 54.6156\n",
      "[570]\tvalid_0's l1: 12.231\tvalid_0's smape: 54.6073\n",
      "[580]\tvalid_0's l1: 12.227\tvalid_0's smape: 54.5966\n",
      "[590]\tvalid_0's l1: 12.2219\tvalid_0's smape: 54.5772\n",
      "[600]\tvalid_0's l1: 12.2166\tvalid_0's smape: 54.5577\n",
      "[610]\tvalid_0's l1: 12.2126\tvalid_0's smape: 54.5356\n",
      "[620]\tvalid_0's l1: 12.2102\tvalid_0's smape: 54.5309\n",
      "[630]\tvalid_0's l1: 12.2077\tvalid_0's smape: 54.5206\n",
      "[640]\tvalid_0's l1: 12.2028\tvalid_0's smape: 54.5054\n",
      "[650]\tvalid_0's l1: 12.197\tvalid_0's smape: 54.4866\n",
      "[660]\tvalid_0's l1: 12.1945\tvalid_0's smape: 54.476\n",
      "[670]\tvalid_0's l1: 12.1848\tvalid_0's smape: 54.4532\n",
      "[680]\tvalid_0's l1: 12.1813\tvalid_0's smape: 54.443\n",
      "[690]\tvalid_0's l1: 12.1752\tvalid_0's smape: 54.4254\n",
      "[700]\tvalid_0's l1: 12.1721\tvalid_0's smape: 54.4168\n",
      "[710]\tvalid_0's l1: 12.1683\tvalid_0's smape: 54.4038\n",
      "[720]\tvalid_0's l1: 12.1646\tvalid_0's smape: 54.3883\n",
      "[730]\tvalid_0's l1: 12.1558\tvalid_0's smape: 54.367\n",
      "[740]\tvalid_0's l1: 12.154\tvalid_0's smape: 54.3645\n",
      "[750]\tvalid_0's l1: 12.1531\tvalid_0's smape: 54.3684\n",
      "[760]\tvalid_0's l1: 12.15\tvalid_0's smape: 54.3593\n",
      "[770]\tvalid_0's l1: 12.1459\tvalid_0's smape: 54.3457\n",
      "[780]\tvalid_0's l1: 12.1432\tvalid_0's smape: 54.3354\n",
      "[790]\tvalid_0's l1: 12.1421\tvalid_0's smape: 54.3343\n",
      "[800]\tvalid_0's l1: 12.1404\tvalid_0's smape: 54.3236\n",
      "[810]\tvalid_0's l1: 12.1401\tvalid_0's smape: 54.3257\n",
      "[820]\tvalid_0's l1: 12.1395\tvalid_0's smape: 54.3241\n",
      "[830]\tvalid_0's l1: 12.1382\tvalid_0's smape: 54.315\n",
      "[840]\tvalid_0's l1: 12.1284\tvalid_0's smape: 54.2795\n",
      "[850]\tvalid_0's l1: 12.1238\tvalid_0's smape: 54.274\n",
      "[860]\tvalid_0's l1: 12.1162\tvalid_0's smape: 54.2522\n",
      "[870]\tvalid_0's l1: 12.1145\tvalid_0's smape: 54.2372\n",
      "[880]\tvalid_0's l1: 12.1099\tvalid_0's smape: 54.2283\n",
      "[890]\tvalid_0's l1: 12.1057\tvalid_0's smape: 54.2172\n",
      "[900]\tvalid_0's l1: 12.1027\tvalid_0's smape: 54.2054\n",
      "[910]\tvalid_0's l1: 12.1018\tvalid_0's smape: 54.202\n",
      "[920]\tvalid_0's l1: 12.0995\tvalid_0's smape: 54.1938\n",
      "[930]\tvalid_0's l1: 12.098\tvalid_0's smape: 54.1862\n",
      "[940]\tvalid_0's l1: 12.0968\tvalid_0's smape: 54.1839\n",
      "[950]\tvalid_0's l1: 12.0925\tvalid_0's smape: 54.1673\n",
      "[960]\tvalid_0's l1: 12.0838\tvalid_0's smape: 54.1479\n",
      "[970]\tvalid_0's l1: 12.0826\tvalid_0's smape: 54.1442\n",
      "[980]\tvalid_0's l1: 12.0814\tvalid_0's smape: 54.1379\n",
      "[990]\tvalid_0's l1: 12.0809\tvalid_0's smape: 54.1346\n",
      "[1000]\tvalid_0's l1: 12.0773\tvalid_0's smape: 54.1128\n",
      "[1010]\tvalid_0's l1: 12.0749\tvalid_0's smape: 54.1025\n",
      "[1020]\tvalid_0's l1: 12.0737\tvalid_0's smape: 54.0966\n",
      "[1030]\tvalid_0's l1: 12.0706\tvalid_0's smape: 54.076\n",
      "[1040]\tvalid_0's l1: 12.0683\tvalid_0's smape: 54.0643\n",
      "[1050]\tvalid_0's l1: 12.0632\tvalid_0's smape: 54.0387\n",
      "[1060]\tvalid_0's l1: 12.0594\tvalid_0's smape: 54.0223\n",
      "[1070]\tvalid_0's l1: 12.0512\tvalid_0's smape: 54.0111\n",
      "[1080]\tvalid_0's l1: 12.0455\tvalid_0's smape: 54.0051\n",
      "[1090]\tvalid_0's l1: 12.0403\tvalid_0's smape: 53.9979\n",
      "[1100]\tvalid_0's l1: 12.0363\tvalid_0's smape: 53.9833\n",
      "[1110]\tvalid_0's l1: 12.0318\tvalid_0's smape: 53.9568\n",
      "[1120]\tvalid_0's l1: 12.0257\tvalid_0's smape: 53.9386\n",
      "[1130]\tvalid_0's l1: 12.0249\tvalid_0's smape: 53.9377\n",
      "[1140]\tvalid_0's l1: 12.0227\tvalid_0's smape: 53.9241\n",
      "[1150]\tvalid_0's l1: 12.0196\tvalid_0's smape: 53.9068\n",
      "[1160]\tvalid_0's l1: 12.0197\tvalid_0's smape: 53.9091\n",
      "[1170]\tvalid_0's l1: 12.0171\tvalid_0's smape: 53.9017\n",
      "[1180]\tvalid_0's l1: 12.0143\tvalid_0's smape: 53.8928\n",
      "[1190]\tvalid_0's l1: 12.0123\tvalid_0's smape: 53.888\n",
      "[1200]\tvalid_0's l1: 12.0103\tvalid_0's smape: 53.884\n",
      "[1210]\tvalid_0's l1: 12.0083\tvalid_0's smape: 53.8799\n",
      "[1220]\tvalid_0's l1: 12.0065\tvalid_0's smape: 53.87\n",
      "[1230]\tvalid_0's l1: 12.0056\tvalid_0's smape: 53.8665\n",
      "[1240]\tvalid_0's l1: 12.0049\tvalid_0's smape: 53.8648\n",
      "[1250]\tvalid_0's l1: 12.0042\tvalid_0's smape: 53.8671\n",
      "[1260]\tvalid_0's l1: 12.0042\tvalid_0's smape: 53.8664\n",
      "[1270]\tvalid_0's l1: 12.0039\tvalid_0's smape: 53.8646\n",
      "[1280]\tvalid_0's l1: 12.0029\tvalid_0's smape: 53.8622\n",
      "[1290]\tvalid_0's l1: 12.0016\tvalid_0's smape: 53.86\n",
      "[1300]\tvalid_0's l1: 12.0007\tvalid_0's smape: 53.8555\n",
      "[1310]\tvalid_0's l1: 11.9991\tvalid_0's smape: 53.8525\n",
      "[1320]\tvalid_0's l1: 11.9978\tvalid_0's smape: 53.8482\n",
      "[1330]\tvalid_0's l1: 11.9975\tvalid_0's smape: 53.8475\n",
      "[1340]\tvalid_0's l1: 11.9969\tvalid_0's smape: 53.8424\n",
      "[1350]\tvalid_0's l1: 11.9965\tvalid_0's smape: 53.8438\n",
      "[1360]\tvalid_0's l1: 11.9961\tvalid_0's smape: 53.845\n",
      "[1370]\tvalid_0's l1: 11.996\tvalid_0's smape: 53.8473\n",
      "[1380]\tvalid_0's l1: 11.9955\tvalid_0's smape: 53.8473\n",
      "[1390]\tvalid_0's l1: 11.9946\tvalid_0's smape: 53.8422\n",
      "[1400]\tvalid_0's l1: 11.9936\tvalid_0's smape: 53.8348\n",
      "[1410]\tvalid_0's l1: 11.9925\tvalid_0's smape: 53.8311\n",
      "[1420]\tvalid_0's l1: 11.9922\tvalid_0's smape: 53.828\n",
      "[1430]\tvalid_0's l1: 11.992\tvalid_0's smape: 53.8285\n",
      "[1440]\tvalid_0's l1: 11.991\tvalid_0's smape: 53.8241\n",
      "[1450]\tvalid_0's l1: 11.9907\tvalid_0's smape: 53.821\n",
      "[1460]\tvalid_0's l1: 11.9907\tvalid_0's smape: 53.8206\n",
      "[1470]\tvalid_0's l1: 11.9903\tvalid_0's smape: 53.8174\n",
      "[1480]\tvalid_0's l1: 11.9903\tvalid_0's smape: 53.8204\n",
      "[1490]\tvalid_0's l1: 11.9894\tvalid_0's smape: 53.8167\n",
      "[1500]\tvalid_0's l1: 11.9893\tvalid_0's smape: 53.8185\n",
      "[1510]\tvalid_0's l1: 11.9886\tvalid_0's smape: 53.8174\n",
      "[1520]\tvalid_0's l1: 11.9876\tvalid_0's smape: 53.8139\n",
      "[1530]\tvalid_0's l1: 11.9865\tvalid_0's smape: 53.8102\n",
      "[1540]\tvalid_0's l1: 11.9856\tvalid_0's smape: 53.8092\n",
      "[1550]\tvalid_0's l1: 11.9846\tvalid_0's smape: 53.8048\n",
      "[1560]\tvalid_0's l1: 11.9835\tvalid_0's smape: 53.801\n",
      "[1570]\tvalid_0's l1: 11.9819\tvalid_0's smape: 53.801\n",
      "[1580]\tvalid_0's l1: 11.9806\tvalid_0's smape: 53.7961\n",
      "[1590]\tvalid_0's l1: 11.9793\tvalid_0's smape: 53.7923\n",
      "[1600]\tvalid_0's l1: 11.9781\tvalid_0's smape: 53.7954\n",
      "[1610]\tvalid_0's l1: 11.9774\tvalid_0's smape: 53.7939\n",
      "[1620]\tvalid_0's l1: 11.9768\tvalid_0's smape: 53.7916\n",
      "[1630]\tvalid_0's l1: 11.9759\tvalid_0's smape: 53.7895\n",
      "[1640]\tvalid_0's l1: 11.9758\tvalid_0's smape: 53.7904\n",
      "[1650]\tvalid_0's l1: 11.9756\tvalid_0's smape: 53.7922\n",
      "[1660]\tvalid_0's l1: 11.9749\tvalid_0's smape: 53.7914\n",
      "[1670]\tvalid_0's l1: 11.9731\tvalid_0's smape: 53.7832\n",
      "[1680]\tvalid_0's l1: 11.9712\tvalid_0's smape: 53.7767\n",
      "[1690]\tvalid_0's l1: 11.9689\tvalid_0's smape: 53.7727\n",
      "[1700]\tvalid_0's l1: 11.9691\tvalid_0's smape: 53.7764\n",
      "[1710]\tvalid_0's l1: 11.9681\tvalid_0's smape: 53.7724\n",
      "[1720]\tvalid_0's l1: 11.968\tvalid_0's smape: 53.7759\n",
      "[1730]\tvalid_0's l1: 11.9682\tvalid_0's smape: 53.7758\n",
      "[1740]\tvalid_0's l1: 11.968\tvalid_0's smape: 53.7742\n",
      "[1750]\tvalid_0's l1: 11.9673\tvalid_0's smape: 53.7696\n",
      "[1760]\tvalid_0's l1: 11.9666\tvalid_0's smape: 53.7689\n",
      "[1770]\tvalid_0's l1: 11.966\tvalid_0's smape: 53.767\n",
      "[1780]\tvalid_0's l1: 11.9661\tvalid_0's smape: 53.771\n",
      "[1790]\tvalid_0's l1: 11.9656\tvalid_0's smape: 53.7685\n",
      "[1800]\tvalid_0's l1: 11.9655\tvalid_0's smape: 53.7684\n",
      "[1810]\tvalid_0's l1: 11.9654\tvalid_0's smape: 53.7684\n",
      "[1820]\tvalid_0's l1: 11.965\tvalid_0's smape: 53.7671\n",
      "[1830]\tvalid_0's l1: 11.9648\tvalid_0's smape: 53.7654\n",
      "[1840]\tvalid_0's l1: 11.9644\tvalid_0's smape: 53.7597\n",
      "[1850]\tvalid_0's l1: 11.9642\tvalid_0's smape: 53.7603\n",
      "[1860]\tvalid_0's l1: 11.9623\tvalid_0's smape: 53.7533\n",
      "[1870]\tvalid_0's l1: 11.9618\tvalid_0's smape: 53.7526\n",
      "[1880]\tvalid_0's l1: 11.9611\tvalid_0's smape: 53.7484\n",
      "[1890]\tvalid_0's l1: 11.9605\tvalid_0's smape: 53.7455\n",
      "[1900]\tvalid_0's l1: 11.9587\tvalid_0's smape: 53.7386\n",
      "[1910]\tvalid_0's l1: 11.9576\tvalid_0's smape: 53.7367\n",
      "[1920]\tvalid_0's l1: 11.9575\tvalid_0's smape: 53.7373\n",
      "[1930]\tvalid_0's l1: 11.9575\tvalid_0's smape: 53.7388\n",
      "[1940]\tvalid_0's l1: 11.9576\tvalid_0's smape: 53.7391\n",
      "[1950]\tvalid_0's l1: 11.9575\tvalid_0's smape: 53.738\n",
      "[1960]\tvalid_0's l1: 11.9572\tvalid_0's smape: 53.7344\n",
      "[1970]\tvalid_0's l1: 11.9571\tvalid_0's smape: 53.7353\n",
      "[1980]\tvalid_0's l1: 11.9568\tvalid_0's smape: 53.7381\n",
      "[1990]\tvalid_0's l1: 11.9565\tvalid_0's smape: 53.7348\n",
      "[2000]\tvalid_0's l1: 11.9557\tvalid_0's smape: 53.729\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 11.9557\tvalid_0's smape: 53.729\n",
      "\n",
      "Evaluating model on validation set...\n",
      "Validation Mean Absolute Error: 11.9557\n",
      "\n",
      "Retraining model on full training data for final submission...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.648318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 803292\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 5266\n",
      "[LightGBM] [Info] Start training from score 14.000000\n",
      "[10]\ttraining's l1: 15.1444\ttraining's smape: 66.3833\n",
      "[20]\ttraining's l1: 14.3593\ttraining's smape: 63.2185\n",
      "[30]\ttraining's l1: 13.8467\ttraining's smape: 61.2452\n",
      "[40]\ttraining's l1: 13.4682\ttraining's smape: 59.8251\n",
      "[50]\ttraining's l1: 13.164\ttraining's smape: 58.7\n",
      "[60]\ttraining's l1: 12.9597\ttraining's smape: 57.8808\n",
      "[70]\ttraining's l1: 12.7892\ttraining's smape: 57.1973\n",
      "[80]\ttraining's l1: 12.6492\ttraining's smape: 56.5721\n",
      "[90]\ttraining's l1: 12.5296\ttraining's smape: 56.0575\n",
      "[100]\ttraining's l1: 12.4201\ttraining's smape: 55.5789\n",
      "[110]\ttraining's l1: 12.3271\ttraining's smape: 55.2091\n",
      "[120]\ttraining's l1: 12.2352\ttraining's smape: 54.8309\n",
      "[130]\ttraining's l1: 12.1599\ttraining's smape: 54.5068\n",
      "[140]\ttraining's l1: 12.0859\ttraining's smape: 54.1837\n",
      "[150]\ttraining's l1: 12.019\ttraining's smape: 53.9069\n",
      "[160]\ttraining's l1: 11.9509\ttraining's smape: 53.634\n",
      "[170]\ttraining's l1: 11.8953\ttraining's smape: 53.3685\n",
      "[180]\ttraining's l1: 11.8421\ttraining's smape: 53.1349\n",
      "[190]\ttraining's l1: 11.7958\ttraining's smape: 52.9264\n",
      "[200]\ttraining's l1: 11.7501\ttraining's smape: 52.7038\n",
      "[210]\ttraining's l1: 11.7034\ttraining's smape: 52.5027\n",
      "[220]\ttraining's l1: 11.6666\ttraining's smape: 52.3248\n",
      "[230]\ttraining's l1: 11.6236\ttraining's smape: 52.1208\n",
      "[240]\ttraining's l1: 11.5834\ttraining's smape: 51.9406\n",
      "[250]\ttraining's l1: 11.548\ttraining's smape: 51.7713\n",
      "[260]\ttraining's l1: 11.5145\ttraining's smape: 51.5979\n",
      "[270]\ttraining's l1: 11.4837\ttraining's smape: 51.4309\n",
      "[280]\ttraining's l1: 11.4475\ttraining's smape: 51.2502\n",
      "[290]\ttraining's l1: 11.416\ttraining's smape: 51.0928\n",
      "[300]\ttraining's l1: 11.3856\ttraining's smape: 50.9327\n",
      "[310]\ttraining's l1: 11.3498\ttraining's smape: 50.7806\n",
      "[320]\ttraining's l1: 11.3204\ttraining's smape: 50.6335\n",
      "[330]\ttraining's l1: 11.2923\ttraining's smape: 50.4909\n",
      "[340]\ttraining's l1: 11.2676\ttraining's smape: 50.3481\n",
      "[350]\ttraining's l1: 11.2438\ttraining's smape: 50.1987\n",
      "[360]\ttraining's l1: 11.2224\ttraining's smape: 50.0686\n",
      "[370]\ttraining's l1: 11.1959\ttraining's smape: 49.9254\n",
      "[380]\ttraining's l1: 11.1727\ttraining's smape: 49.7977\n",
      "[390]\ttraining's l1: 11.1537\ttraining's smape: 49.6877\n",
      "[400]\ttraining's l1: 11.1289\ttraining's smape: 49.5657\n",
      "[410]\ttraining's l1: 11.1075\ttraining's smape: 49.4481\n",
      "[420]\ttraining's l1: 11.0916\ttraining's smape: 49.3442\n",
      "[430]\ttraining's l1: 11.0684\ttraining's smape: 49.2229\n",
      "[440]\ttraining's l1: 11.0481\ttraining's smape: 49.1007\n",
      "[450]\ttraining's l1: 11.0258\ttraining's smape: 48.9902\n",
      "[460]\ttraining's l1: 11.0067\ttraining's smape: 48.8851\n",
      "[470]\ttraining's l1: 10.9843\ttraining's smape: 48.7792\n",
      "[480]\ttraining's l1: 10.9676\ttraining's smape: 48.6754\n",
      "[490]\ttraining's l1: 10.9529\ttraining's smape: 48.5883\n",
      "[500]\ttraining's l1: 10.9373\ttraining's smape: 48.4998\n",
      "[510]\ttraining's l1: 10.921\ttraining's smape: 48.3981\n",
      "[520]\ttraining's l1: 10.9049\ttraining's smape: 48.3009\n",
      "[530]\ttraining's l1: 10.8839\ttraining's smape: 48.1789\n",
      "[540]\ttraining's l1: 10.8679\ttraining's smape: 48.0914\n",
      "[550]\ttraining's l1: 10.8526\ttraining's smape: 48.0023\n",
      "[560]\ttraining's l1: 10.8393\ttraining's smape: 47.9184\n",
      "[570]\ttraining's l1: 10.8264\ttraining's smape: 47.838\n",
      "[580]\ttraining's l1: 10.8105\ttraining's smape: 47.7375\n",
      "[590]\ttraining's l1: 10.7908\ttraining's smape: 47.6356\n",
      "[600]\ttraining's l1: 10.7727\ttraining's smape: 47.5366\n",
      "[610]\ttraining's l1: 10.7529\ttraining's smape: 47.4385\n",
      "[620]\ttraining's l1: 10.7367\ttraining's smape: 47.355\n",
      "[630]\ttraining's l1: 10.7191\ttraining's smape: 47.2744\n",
      "[640]\ttraining's l1: 10.7043\ttraining's smape: 47.1988\n",
      "[650]\ttraining's l1: 10.693\ttraining's smape: 47.1297\n",
      "[660]\ttraining's l1: 10.6722\ttraining's smape: 47.0398\n",
      "[670]\ttraining's l1: 10.6591\ttraining's smape: 46.9607\n",
      "[680]\ttraining's l1: 10.6462\ttraining's smape: 46.8855\n",
      "[690]\ttraining's l1: 10.634\ttraining's smape: 46.8221\n",
      "[700]\ttraining's l1: 10.6219\ttraining's smape: 46.7504\n",
      "[710]\ttraining's l1: 10.6115\ttraining's smape: 46.6833\n",
      "[720]\ttraining's l1: 10.6019\ttraining's smape: 46.6231\n",
      "[730]\ttraining's l1: 10.5909\ttraining's smape: 46.5571\n",
      "[740]\ttraining's l1: 10.5793\ttraining's smape: 46.4867\n",
      "[750]\ttraining's l1: 10.5675\ttraining's smape: 46.4202\n",
      "[760]\ttraining's l1: 10.5573\ttraining's smape: 46.3556\n",
      "[770]\ttraining's l1: 10.5493\ttraining's smape: 46.3116\n",
      "[780]\ttraining's l1: 10.5402\ttraining's smape: 46.2602\n",
      "[790]\ttraining's l1: 10.5317\ttraining's smape: 46.2116\n",
      "[800]\ttraining's l1: 10.5251\ttraining's smape: 46.1759\n",
      "[810]\ttraining's l1: 10.5181\ttraining's smape: 46.1359\n",
      "[820]\ttraining's l1: 10.5052\ttraining's smape: 46.0613\n",
      "[830]\ttraining's l1: 10.4878\ttraining's smape: 45.9714\n",
      "[840]\ttraining's l1: 10.4707\ttraining's smape: 45.8817\n",
      "[850]\ttraining's l1: 10.4585\ttraining's smape: 45.803\n",
      "[860]\ttraining's l1: 10.4447\ttraining's smape: 45.7243\n",
      "[870]\ttraining's l1: 10.4339\ttraining's smape: 45.6523\n",
      "[880]\ttraining's l1: 10.4223\ttraining's smape: 45.5772\n",
      "[890]\ttraining's l1: 10.4097\ttraining's smape: 45.5008\n",
      "[900]\ttraining's l1: 10.3978\ttraining's smape: 45.4354\n",
      "[910]\ttraining's l1: 10.3863\ttraining's smape: 45.365\n",
      "[920]\ttraining's l1: 10.3735\ttraining's smape: 45.2871\n",
      "[930]\ttraining's l1: 10.3626\ttraining's smape: 45.2197\n",
      "[940]\ttraining's l1: 10.3514\ttraining's smape: 45.159\n",
      "[950]\ttraining's l1: 10.341\ttraining's smape: 45.0947\n",
      "[960]\ttraining's l1: 10.3303\ttraining's smape: 45.0339\n",
      "[970]\ttraining's l1: 10.322\ttraining's smape: 44.9824\n",
      "[980]\ttraining's l1: 10.3128\ttraining's smape: 44.9372\n",
      "[990]\ttraining's l1: 10.2982\ttraining's smape: 44.8626\n",
      "[1000]\ttraining's l1: 10.2895\ttraining's smape: 44.8133\n",
      "[1010]\ttraining's l1: 10.2774\ttraining's smape: 44.7436\n",
      "[1020]\ttraining's l1: 10.2672\ttraining's smape: 44.6844\n",
      "[1030]\ttraining's l1: 10.2556\ttraining's smape: 44.6176\n",
      "[1040]\ttraining's l1: 10.2474\ttraining's smape: 44.5613\n",
      "[1050]\ttraining's l1: 10.2355\ttraining's smape: 44.4859\n",
      "[1060]\ttraining's l1: 10.2258\ttraining's smape: 44.4278\n",
      "[1070]\ttraining's l1: 10.2138\ttraining's smape: 44.3622\n",
      "[1080]\ttraining's l1: 10.2024\ttraining's smape: 44.2967\n",
      "[1090]\ttraining's l1: 10.1906\ttraining's smape: 44.2297\n",
      "[1100]\ttraining's l1: 10.1794\ttraining's smape: 44.167\n",
      "[1110]\ttraining's l1: 10.1719\ttraining's smape: 44.1211\n",
      "[1120]\ttraining's l1: 10.1652\ttraining's smape: 44.077\n",
      "[1130]\ttraining's l1: 10.1557\ttraining's smape: 44.022\n",
      "[1140]\ttraining's l1: 10.1468\ttraining's smape: 43.9688\n",
      "[1150]\ttraining's l1: 10.1373\ttraining's smape: 43.8999\n",
      "[1160]\ttraining's l1: 10.1282\ttraining's smape: 43.8465\n",
      "[1170]\ttraining's l1: 10.1199\ttraining's smape: 43.7917\n",
      "[1180]\ttraining's l1: 10.1099\ttraining's smape: 43.7357\n",
      "[1190]\ttraining's l1: 10.1025\ttraining's smape: 43.6837\n",
      "[1200]\ttraining's l1: 10.0963\ttraining's smape: 43.6437\n",
      "[1210]\ttraining's l1: 10.0891\ttraining's smape: 43.6004\n",
      "[1220]\ttraining's l1: 10.0818\ttraining's smape: 43.5551\n",
      "[1230]\ttraining's l1: 10.0752\ttraining's smape: 43.5152\n",
      "[1240]\ttraining's l1: 10.0668\ttraining's smape: 43.4606\n",
      "[1250]\ttraining's l1: 10.0586\ttraining's smape: 43.4149\n",
      "[1260]\ttraining's l1: 10.0518\ttraining's smape: 43.3753\n",
      "[1270]\ttraining's l1: 10.044\ttraining's smape: 43.3301\n",
      "[1280]\ttraining's l1: 10.0382\ttraining's smape: 43.2965\n",
      "[1290]\ttraining's l1: 10.0327\ttraining's smape: 43.2638\n",
      "[1300]\ttraining's l1: 10.0265\ttraining's smape: 43.2293\n",
      "[1310]\ttraining's l1: 10.0201\ttraining's smape: 43.1869\n",
      "[1320]\ttraining's l1: 10.0084\ttraining's smape: 43.1341\n",
      "[1330]\ttraining's l1: 9.99399\ttraining's smape: 43.0804\n",
      "[1340]\ttraining's l1: 9.98374\ttraining's smape: 43.0232\n",
      "[1350]\ttraining's l1: 9.97486\ttraining's smape: 42.9762\n",
      "[1360]\ttraining's l1: 9.9674\ttraining's smape: 42.929\n",
      "[1370]\ttraining's l1: 9.96017\ttraining's smape: 42.8931\n",
      "[1380]\ttraining's l1: 9.9518\ttraining's smape: 42.8544\n",
      "[1390]\ttraining's l1: 9.94554\ttraining's smape: 42.8181\n",
      "[1400]\ttraining's l1: 9.93862\ttraining's smape: 42.7768\n",
      "[1410]\ttraining's l1: 9.93125\ttraining's smape: 42.736\n",
      "[1420]\ttraining's l1: 9.92424\ttraining's smape: 42.6979\n",
      "[1430]\ttraining's l1: 9.91725\ttraining's smape: 42.6539\n",
      "[1440]\ttraining's l1: 9.90934\ttraining's smape: 42.6025\n",
      "[1450]\ttraining's l1: 9.90234\ttraining's smape: 42.5621\n",
      "[1460]\ttraining's l1: 9.89494\ttraining's smape: 42.5228\n",
      "[1470]\ttraining's l1: 9.88926\ttraining's smape: 42.4883\n",
      "[1480]\ttraining's l1: 9.88087\ttraining's smape: 42.4388\n",
      "[1490]\ttraining's l1: 9.87314\ttraining's smape: 42.3911\n",
      "[1500]\ttraining's l1: 9.86555\ttraining's smape: 42.3509\n",
      "[1510]\ttraining's l1: 9.85767\ttraining's smape: 42.3088\n",
      "[1520]\ttraining's l1: 9.85176\ttraining's smape: 42.2771\n",
      "[1530]\ttraining's l1: 9.84361\ttraining's smape: 42.2346\n",
      "[1540]\ttraining's l1: 9.83818\ttraining's smape: 42.2056\n",
      "[1550]\ttraining's l1: 9.83051\ttraining's smape: 42.1692\n",
      "[1560]\ttraining's l1: 9.82417\ttraining's smape: 42.1339\n",
      "[1570]\ttraining's l1: 9.81631\ttraining's smape: 42.0909\n",
      "[1580]\ttraining's l1: 9.80866\ttraining's smape: 42.0501\n",
      "[1590]\ttraining's l1: 9.80349\ttraining's smape: 42.0211\n",
      "[1600]\ttraining's l1: 9.79659\ttraining's smape: 41.9939\n",
      "[1610]\ttraining's l1: 9.79132\ttraining's smape: 41.9642\n",
      "[1620]\ttraining's l1: 9.78658\ttraining's smape: 41.9388\n",
      "[1630]\ttraining's l1: 9.78063\ttraining's smape: 41.9109\n",
      "[1640]\ttraining's l1: 9.77574\ttraining's smape: 41.8855\n",
      "[1650]\ttraining's l1: 9.76941\ttraining's smape: 41.8539\n",
      "[1660]\ttraining's l1: 9.76516\ttraining's smape: 41.8294\n",
      "[1670]\ttraining's l1: 9.76024\ttraining's smape: 41.8057\n",
      "[1680]\ttraining's l1: 9.75522\ttraining's smape: 41.777\n",
      "[1690]\ttraining's l1: 9.75073\ttraining's smape: 41.7552\n",
      "[1700]\ttraining's l1: 9.74675\ttraining's smape: 41.7338\n",
      "[1710]\ttraining's l1: 9.74131\ttraining's smape: 41.7027\n",
      "[1720]\ttraining's l1: 9.73663\ttraining's smape: 41.6683\n",
      "[1730]\ttraining's l1: 9.73167\ttraining's smape: 41.637\n",
      "[1740]\ttraining's l1: 9.72643\ttraining's smape: 41.6046\n",
      "[1750]\ttraining's l1: 9.72007\ttraining's smape: 41.5697\n",
      "[1760]\ttraining's l1: 9.71404\ttraining's smape: 41.5298\n",
      "[1770]\ttraining's l1: 9.70509\ttraining's smape: 41.4719\n",
      "[1780]\ttraining's l1: 9.69957\ttraining's smape: 41.4292\n",
      "[1790]\ttraining's l1: 9.69243\ttraining's smape: 41.3808\n",
      "[1800]\ttraining's l1: 9.68645\ttraining's smape: 41.3399\n",
      "[1810]\ttraining's l1: 9.68136\ttraining's smape: 41.305\n",
      "[1820]\ttraining's l1: 9.67394\ttraining's smape: 41.2638\n",
      "[1830]\ttraining's l1: 9.66936\ttraining's smape: 41.2376\n",
      "[1840]\ttraining's l1: 9.66484\ttraining's smape: 41.2094\n",
      "[1850]\ttraining's l1: 9.65971\ttraining's smape: 41.182\n",
      "[1860]\ttraining's l1: 9.65497\ttraining's smape: 41.1528\n",
      "[1870]\ttraining's l1: 9.65058\ttraining's smape: 41.1244\n",
      "[1880]\ttraining's l1: 9.64521\ttraining's smape: 41.0892\n",
      "[1890]\ttraining's l1: 9.64004\ttraining's smape: 41.0516\n",
      "[1900]\ttraining's l1: 9.63541\ttraining's smape: 41.0211\n",
      "[1910]\ttraining's l1: 9.63006\ttraining's smape: 40.9846\n",
      "[1920]\ttraining's l1: 9.62442\ttraining's smape: 40.9427\n",
      "[1930]\ttraining's l1: 9.619\ttraining's smape: 40.9093\n",
      "[1940]\ttraining's l1: 9.61395\ttraining's smape: 40.8841\n",
      "[1950]\ttraining's l1: 9.60903\ttraining's smape: 40.8569\n",
      "[1960]\ttraining's l1: 9.60542\ttraining's smape: 40.8361\n",
      "[1970]\ttraining's l1: 9.60087\ttraining's smape: 40.8109\n",
      "[1980]\ttraining's l1: 9.59374\ttraining's smape: 40.7729\n",
      "[1990]\ttraining's l1: 9.58839\ttraining's smape: 40.7384\n",
      "[2000]\ttraining's l1: 9.58232\ttraining's smape: 40.7031\n",
      "\n",
      "Predicting on test data...\n",
      "\n",
      "✅ Submission file 'submission.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSplitting data for validation...\")\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_full, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}\")\n",
    "\n",
    "print(\"\\nTraining LightGBM model with validation...\")\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train, y_train_split,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=smape_metric,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100, verbose=True),\n",
    "        lgb.log_evaluation(period=10)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating model on validation set...\")\n",
    "val_preds = lgbm.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, val_preds)\n",
    "print(f\"Validation Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "print(\"\\nRetraining model on full training data for final submission...\")\n",
    "lgbm_final = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    n_estimators=lgbm.best_iteration_,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_final.fit(X_train_full, y_train,\n",
    "                       eval_set=[(X_train_full, y_train)],\n",
    "                       eval_metric=smape_metric,\n",
    "                       callbacks=[lgb.log_evaluation(period=10)])\n",
    "\n",
    "print(\"\\nPredicting on test data...\")\n",
    "predictions = lgbm_final.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'sample_id': test_ids, 'price': predictions})\n",
    "submission['price'] = submission['price'].clip(0)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Submission file 'submission.csv' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
